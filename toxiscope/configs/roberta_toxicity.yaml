# ToxiScope Configuration: RoBERTa-base (Jigsaw fine-tuned)
# Alternative: Pre-trained on Jigsaw toxicity data
# Reference: Hanu & Unitary (2020) "Detoxify" - https://github.com/unitaryai/detoxify

model:
  # Options:
  # - "unitary/toxic-bert" - BERT fine-tuned on Jigsaw
  # - "unitary/unbiased-toxic-roberta" - RoBERTa with bias mitigation
  # - "roberta-base" - Train from scratch
  name: "roberta-base"
  num_labels: 7
  problem_type: "multi_label_classification"
  
  # ToxiScope enhancements
  pooling_strategy: "cls"
  use_multi_sample_dropout: true
  dropout_samples: 5
  hidden_dropout_prob: 0.1
  attention_probs_dropout_prob: 0.1
  classifier_dropout: 0.2

data:
  train_path: "data/processed/train.csv"
  val_path: "data/processed/val.csv"
  test_path: "data/processed/test.csv"
  text_column: "body_clean"
  label_columns:
    - "toxic"
    - "severe_toxic"
    - "obscene"
    - "threat"
    - "insult"
    - "identity_hate"
    - "racism"
  max_length: 256

training:
  output_dir: "outputs/models/roberta_base"
  num_train_epochs: 5
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 2
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  
  loss_function: "focal"
  focal_gamma: 2.0
  focal_alpha: 0.25
  
  fp16: true
  dataloader_num_workers: 4
  
  eval_strategy: "steps"
  eval_steps: 500
  save_strategy: "steps"
  save_steps: 500
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "macro_f1"
  greater_is_better: true
  
  early_stopping_patience: 3
  early_stopping_threshold: 0.001

cross_validation:
  enabled: true
  n_folds: 5
  stratified: true

threshold_calibration:
  enabled: true
  method: "f1_optimal"
  search_range: [0.1, 0.95]
  search_steps: 50

wandb:
  project: "toxiscope"
  entity: null
  run_name: "roberta-base"
  tags: ["roberta", "toxicity", "multilabel", "gaming"]

seed: 42
