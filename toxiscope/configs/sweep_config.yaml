# WandB Hyperparameter Sweep Configuration
# For IndoBERT optimization

program: src/train.py
method: bayes  # grid, random, bayes
metric:
  name: val/macro_f1
  goal: maximize

parameters:
  # Learning rate (most impactful)
  learning_rate:
    distribution: log_uniform_values
    min: 1.0e-6
    max: 5.0e-5
  
  # Batch size
  batch_size:
    values: [8, 16, 32]
  
  # Number of epochs
  num_epochs:
    values: [3, 4, 5, 6]
  
  # Dropout
  dropout:
    distribution: uniform
    min: 0.05
    max: 0.3
  
  # Warmup ratio
  warmup_ratio:
    values: [0.0, 0.05, 0.1, 0.15]
  
  # Weight decay
  weight_decay:
    distribution: log_uniform_values
    min: 1.0e-4
    max: 0.1
  
  # Focal loss gamma
  focal_gamma:
    values: [1.0, 1.5, 2.0, 2.5, 3.0]
  
  # Label smoothing
  label_smoothing:
    values: [0.0, 0.05, 0.1, 0.15]
  
  # Scheduler type
  scheduler:
    values: ["linear", "cosine", "cosine_with_restarts"]
  
  # Pooling strategy
  pooling:
    values: ["cls", "mean", "attention"]

# Early termination
early_terminate:
  type: hyperband
  min_iter: 2
  eta: 3
  s: 2

# Number of runs
run_cap: 50

# Command template
command:
  - ${env}
  - python
  - ${program}
  - --config
  - configs/indobert_base.yaml
  - --sweep
  - ${args}
