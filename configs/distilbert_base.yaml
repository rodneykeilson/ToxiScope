# ToxiScope: DistilBERT Configuration (Memory Efficient)
# DistilBERT is 40% smaller than BERT with 97% of performance
# Ideal for systems with limited RAM/VRAM

model:
  name: "distilbert-base-uncased"
  num_labels: 7
  max_length: 256
  hidden_dropout_prob: 0.2
  attention_probs_dropout_prob: 0.2

training:
  batch_size: 4  # Small batch for memory efficiency
  gradient_accumulation_steps: 4  # Effective batch = 16
  learning_rate: 3e-5
  weight_decay: 0.01
  num_epochs: 3
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  
  # Optimizer
  optimizer: "adamw"
  adam_epsilon: 1e-8
  
  # Scheduler
  scheduler: "linear"
  
  # Loss function
  loss_function: "focal"
  focal_gamma: 2.0
  focal_alpha: 0.25
  
  # Early stopping
  early_stopping_patience: 3
  early_stopping_threshold: 0.001
  
  # Evaluation
  eval_steps: 2000
  save_steps: 2000
  logging_steps: 100
  
  # Memory optimization
  fp16: false  # Disable for CPU stability
  dataloader_num_workers: 0  # Avoid multiprocessing issues on Windows

data:
  train_path: "data/training/processed/train.csv"
  val_path: "data/training/processed/val.csv"
  test_path: "data/training/processed/test.csv"
  text_column: "body_clean"
  label_columns:
    - "toxic"
    - "severe_toxic"
    - "obscene"
    - "threat"
    - "insult"
    - "identity_hate"
    - "racism"

output:
  dir: "outputs/models/distilbert"
  save_total_limit: 2

wandb:
  project: "toxiscope"
  name: "distilbert-base-focal"
  tags: ["distilbert", "multilabel", "toxicity"]
