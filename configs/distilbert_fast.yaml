# ToxiScope: Fast Training Configuration (Sampled Dataset)
# Uses 50K samples for quick training (~1-2 hours on CPU)
# DistilBERT for memory efficiency

model:
  name: "distilbert-base-uncased"
  num_labels: 7
  max_length: 128  # Shorter for faster processing

training:
  batch_size: 8
  gradient_accumulation_steps: 2  # Effective batch = 16
  learning_rate: 5e-5
  weight_decay: 0.01
  num_epochs: 2  # Quick training
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  
  # Optimizer
  optimizer: "adamw"
  adam_epsilon: 1e-8
  
  # Scheduler
  scheduler: "linear"
  
  # Loss function
  loss_function: "focal"
  focal_gamma: 2.0
  focal_alpha: 0.25
  
  # Early stopping
  early_stopping_patience: 2
  early_stopping_threshold: 0.001
  
  # Evaluation
  eval_steps: 500
  save_steps: 500
  logging_steps: 50
  
  # Memory optimization
  fp16: false
  dataloader_num_workers: 0

data:
  train_path: "data/training/sampled/train.csv"
  val_path: "data/training/sampled/val.csv"
  test_path: "data/training/sampled/test.csv"
  text_column: "body_clean"
  label_columns:
    - "toxic"
    - "severe_toxic"
    - "obscene"
    - "threat"
    - "insult"
    - "identity_hate"
    - "racism"

output:
  dir: "outputs/models/distilbert_fast"
  save_total_limit: 2

wandb:
  project: "toxiscope"
  name: "distilbert-fast-sampled"
  tags: ["distilbert", "sampled", "fast"]
